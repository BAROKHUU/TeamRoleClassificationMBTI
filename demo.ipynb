{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07a5662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - (optional) install packages if needed (uncomment to run)\n",
    "!pip install -q gradio sentence-transformers scikit-learn xgboost joblib deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392ca9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in d:\\mbti project\\.venv\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: deep-translator in d:\\mbti project\\.venv\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: six in d:\\mbti project\\.venv\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in d:\\mbti project\\.venv\\lib\\site-packages (from deep-translator) (4.13.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in d:\\mbti project\\.venv\\lib\\site-packages (from deep-translator) (2.32.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\mbti project\\.venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\mbti project\\.venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\mbti project\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\mbti project\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\mbti project\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\mbti project\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\mbti project\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\mbti project\\.venv\\lib\\site-packages (from ipywidgets) (9.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\mbti project\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in d:\\mbti project\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\mbti project\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: wcwidth in d:\\mbti project\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\mbti project\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\mbti project\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in d:\\mbti project\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect deep-translator\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356c8915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready. Torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, io, json, warnings, tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Optional libs (translation / lang detect). Nếu thiếu, thông báo rõ.\n",
    "try:\n",
    "    from deep_translator import GoogleTranslator\n",
    "except Exception:\n",
    "    GoogleTranslator = None\n",
    "    warnings.warn(\"`deep_translator` không cài đặt — chức năng translate sẽ fallback (không dịch).\")\n",
    "\n",
    "try:\n",
    "    from langdetect import detect\n",
    "except Exception:\n",
    "    detect = None\n",
    "    warnings.warn(\"`langdetect` không cài đặt — sẽ dùng heuristic ASCII để kiểm tra ngôn ngữ.\")\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "except Exception:\n",
    "    gr = None\n",
    "    warnings.warn(\"`gradio` không cài đặt. Cài nếu muốn UI local (pip install gradio).\")\n",
    "\n",
    "print(\"Environment ready. Torch device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a28279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedder: all-MiniLM-L6-v2 on cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Embedder (SentenceTransformer)\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedder = SentenceTransformer(MODEL_NAME, device=device)\n",
    "print(\"Loaded embedder:\", MODEL_NAME, \"on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9677b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role mappings loaded. Core roles: ['Leader', 'Planner', 'Executor', 'Facilitator']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - MBTI -> secondary roles, secondary -> main core role mappings\n",
    "CORE_ROLE_NAMES = [\"Leader\", \"Planner\", \"Executor\", \"Facilitator\"]\n",
    "\n",
    "# A plausible mapping MBTI -> secondary roles (2 per MBTI). \n",
    "# Nếu bạn có mapping gốc, thay vào biến này để kết quả giống bản cũ.\n",
    "mbti_roles = {\n",
    "        \"INFP\": [\"Supporter\", \"Idea Generator\"],\n",
    "        \"INFJ\": [\"Leader\", \"Supporter\"],\n",
    "        \"ENFP\": [\"Idea Generator\", \"Communicator\"],\n",
    "        \"ENFJ\": [\"Leader\", \"Supporter\"],\n",
    "        \"INTP\": [\"Idea Generator\", \"Checker\"],\n",
    "        \"INTJ\": [\"Leader\", \"Checker\"],\n",
    "        \"ENTP\": [\"Idea Generator\", \"Leader\"],\n",
    "        \"ENTJ\": [\"Leader\", \"Implementer\"],\n",
    "        \"ISFJ\": [\"Supporter\", \"Finisher\"],\n",
    "        \"ESFJ\": [\"Supporter\", \"Communicator\"],\n",
    "        \"ISTJ\": [\"Checker\", \"Finisher\"],\n",
    "        \"ESTJ\": [\"Leader\", \"Implementer\"],\n",
    "        \"ISFP\": [\"Supporter\", \"Implementer\"],\n",
    "        \"ESFP\": [\"Communicator\", \"Idea Generator\"],\n",
    "        \"ISTP\": [\"Implementer\", \"Problem Solver\"],\n",
    "        \"ESTP\": [\"Leader\", \"Implementer\"],\n",
    "}\n",
    "\n",
    "# Map detailed roles (from mbti_roles) -> 4 main roles\n",
    "# This version is cleaned up to only include roles present in mbti_roles\n",
    "role_to_main = {\n",
    "    # Roles mapping to Leader\n",
    "    \"Leader\": \"Leader\",\n",
    "    # Roles mapping to Planner\n",
    "    \"Idea Generator\": \"Planner\",\n",
    "    \"Checker\": \"Planner\",\n",
    "    # Roles mapping to Executor\n",
    "    \"Implementer\": \"Executor\",\n",
    "    \"Finisher\": \"Executor\",\n",
    "    \"Problem Solver\": \"Executor\",\n",
    "    # Roles mapping to Facilitator\n",
    "    \"Supporter\": \"Facilitator\",\n",
    "    \"Communicator\": \"Facilitator\",\n",
    "}\n",
    "\n",
    "# Ensure every CORE_ROLE_NAMES key exists in role_probs default later\n",
    "# This loop also ensures core roles like \"Leader\" are mapped to themselves\n",
    "for c in CORE_ROLE_NAMES:\n",
    "    role_to_main.setdefault(c, c)\n",
    "\n",
    "print(\"Role mappings loaded. Core roles:\", CORE_ROLE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ee9549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded binary models from folders: ['EI', 'SN', 'TF', 'JP']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Load binary models (tries folder per-axis or a single joblib)\n",
    "def load_binary_models(base_dir=\"src/binary_model\", single_paths=None):\n",
    "    single_paths = single_paths or [\"primary_mbti_clf.joblib\", \"binary_models.joblib\", \"binary_model.joblib\"]\n",
    "    AXES = [\"EI\",\"SN\",\"TF\",\"JP\"]\n",
    "    models = {}\n",
    "    # try per-axis folders\n",
    "    for axis in AXES:\n",
    "        model_path = os.path.join(base_dir, axis, f\"xgb_{axis}.joblib\")\n",
    "        map_path = os.path.join(base_dir, axis, \"label_map.json\")\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                clf = joblib.load(model_path)\n",
    "                id2label = None\n",
    "                if os.path.exists(map_path):\n",
    "                    with open(map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        mm = json.load(f)\n",
    "                    id2label = mm.get(\"id2label\") or mm.get(\"id_to_label\") or mm.get(\"id2label\")\n",
    "                models[axis] = {\"clf\": clf, \"id2label\": id2label}\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Không thể load {model_path}: {e}\")\n",
    "    if len(models) == 4:\n",
    "        print(\"Loaded binary models from folders:\", list(models.keys()))\n",
    "        return models\n",
    "\n",
    "    # try single joblib files\n",
    "    for p in single_paths:\n",
    "        if not os.path.exists(p):\n",
    "            continue\n",
    "        try:\n",
    "            data = joblib.load(p)\n",
    "            if isinstance(data, dict):\n",
    "                # check if dict contains four axis\n",
    "                if all(k in data for k in [\"EI\",\"SN\",\"TF\",\"JP\"]):\n",
    "                    out = {}\n",
    "                    for axis in [\"EI\",\"SN\",\"TF\",\"JP\"]:\n",
    "                        val = data[axis]\n",
    "                        if isinstance(val, dict) and \"clf\" in val:\n",
    "                            out[axis] = {\"clf\": val[\"clf\"], \"id2label\": val.get(\"id2label\")}\n",
    "                        else:\n",
    "                            out[axis] = {\"clf\": val, \"id2label\": None}\n",
    "                    print(f\"Loaded binary models from single joblib: {p}\")\n",
    "                    return out\n",
    "            # if it's a single classifier, assign to EI as fallback\n",
    "            if hasattr(data, \"predict_proba\"):\n",
    "                warnings.warn(f\"Found single model in {p}. Assigned to axis EI as fallback.\")\n",
    "                return {\"EI\": {\"clf\": data, \"id2label\": None}}\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error loading {p}: {e}\")\n",
    "\n",
    "    warnings.warn(\"No binary models found. Inference will fallback to random probs.\")\n",
    "    return {}\n",
    "\n",
    "# Load now\n",
    "binary_models = load_binary_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3fceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - language detect & translate helper + embedding helper\n",
    "def detect_is_english(text):\n",
    "    if not text or text.strip()==\"\":\n",
    "        return True\n",
    "    if detect:\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "            return lang.lower().startswith(\"en\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback heuristic: all ascii => probably english\n",
    "    return all(ord(c) < 128 for c in text)\n",
    "\n",
    "def maybe_translate(text, enable_translate=True, target=\"en\"):\n",
    "    if not enable_translate:\n",
    "        return text\n",
    "    if GoogleTranslator is None:\n",
    "        # no translator lib installed\n",
    "        return text\n",
    "    try:\n",
    "        if detect_is_english(text):\n",
    "            return text\n",
    "        return GoogleTranslator(source='auto', target=target).translate(text)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Translate failed: {e}\")\n",
    "        return text\n",
    "\n",
    "def encode_posts_mean(posts, embedder=embedder, batch_size=32):\n",
    "    parts = [p.strip() for p in str(posts).split(\"|||\") if p.strip()]\n",
    "    if len(parts) == 0:\n",
    "        dim = embedder.get_sentence_embedding_dimension()\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    emb = embedder.encode(parts, convert_to_numpy=True, batch_size=batch_size)\n",
    "    if emb.ndim == 1:\n",
    "        return emb\n",
    "    return emb.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d79bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - inference: axis probs -> 16-type probs -> classify user\n",
    "AXES = [\"EI\",\"SN\",\"TF\",\"JP\"]\n",
    "DEFAULT_LETTERS = {\"EI\":(\"E\",\"I\"), \"SN\":(\"S\",\"N\"), \"TF\":(\"T\",\"F\"), \"JP\":(\"J\",\"P\")}\n",
    "\n",
    "def _ensure_id2label(m):\n",
    "    if m is None:\n",
    "        return None\n",
    "    # id2label may have keys as str; normalize to int keys\n",
    "    try:\n",
    "        return {int(k): v for k,v in m.items()}\n",
    "    except Exception:\n",
    "        return m\n",
    "\n",
    "def predict_mbti_from_embedding(embedding, binary_models=binary_models):\n",
    "    # fallback: uniform random\n",
    "    if not binary_models:\n",
    "        mbti_list = [a+b+c+d for a in [\"E\",\"I\"] for b in [\"S\",\"N\"] for c in [\"T\",\"F\"] for d in [\"J\",\"P\"]]\n",
    "        r = np.random.rand(len(mbti_list))\n",
    "        r /= r.sum()\n",
    "        axis_probs = {axis: {DEFAULT_LETTERS[axis][0]: 0.5, DEFAULT_LETTERS[axis][1]: 0.5} for axis in AXES}\n",
    "        top = mbti_list[int(np.argmax(r))]\n",
    "        return top, float(np.max(r)), axis_probs, dict(zip(mbti_list, r))\n",
    "\n",
    "    axis_probs = {}\n",
    "    for axis in AXES:\n",
    "        info = binary_models.get(axis)\n",
    "        if info is None:\n",
    "            axis_probs[axis] = {DEFAULT_LETTERS[axis][0]:0.5, DEFAULT_LETTERS[axis][1]:0.5}\n",
    "            continue\n",
    "        clf = info[\"clf\"]\n",
    "        id2label = _ensure_id2label(info.get(\"id2label\"))\n",
    "        try:\n",
    "            probs = clf.predict_proba([embedding])[0]\n",
    "            classes = list(clf.classes_)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Predict_proba error for axis {axis}: {e}\")\n",
    "            axis_probs[axis] = {DEFAULT_LETTERS[axis][0]:0.5, DEFAULT_LETTERS[axis][1]:0.5}\n",
    "            continue\n",
    "\n",
    "        this_axis = {}\n",
    "        for idx, cls in enumerate(classes):\n",
    "            # map cls -> letter\n",
    "            mapped = None\n",
    "            if id2label and str(cls) in id2label:\n",
    "                mapped = id2label[str(cls)]\n",
    "            elif isinstance(cls, str) and cls in DEFAULT_LETTERS[axis]:\n",
    "                mapped = cls\n",
    "            else:\n",
    "                try:\n",
    "                    num = int(cls)\n",
    "                    mapped = DEFAULT_LETTERS[axis][num]\n",
    "                except Exception:\n",
    "                    mapped = DEFAULT_LETTERS[axis][idx]\n",
    "            this_axis[mapped] = float(probs[idx])\n",
    "        # ensure both letters exist, normalize\n",
    "        l0,l1 = DEFAULT_LETTERS[axis]\n",
    "        this_axis.setdefault(l0, 1e-9)\n",
    "        this_axis.setdefault(l1, 1e-9)\n",
    "        s = this_axis[l0] + this_axis[l1]\n",
    "        this_axis[l0] /= s; this_axis[l1] /= s\n",
    "        axis_probs[axis] = this_axis\n",
    "\n",
    "    # compute 16-type probability by multiplying axis probabilities\n",
    "    mbti_list = [a+b+c+d for a in [\"E\",\"I\"] for b in [\"S\",\"N\"] for c in [\"T\",\"F\"] for d in [\"J\",\"P\"]]\n",
    "    mbti_probs = {}\n",
    "    for mbti in mbti_list:\n",
    "        p = 1.0\n",
    "        for i, axis in enumerate(AXES):\n",
    "            p *= axis_probs[axis].get(mbti[i], 0.0)\n",
    "        mbti_probs[mbti] = p\n",
    "    total = sum(mbti_probs.values())\n",
    "    if total > 0:\n",
    "        for k in mbti_probs:\n",
    "            mbti_probs[k] /= total\n",
    "    else:\n",
    "        for k in mbti_probs:\n",
    "            mbti_probs[k] = 1.0/len(mbti_probs)\n",
    "\n",
    "    top_mbti = max(mbti_probs.items(), key=lambda x: x[1])[0]\n",
    "    top_conf = float(mbti_probs[top_mbti])\n",
    "    return top_mbti, top_conf, axis_probs, mbti_probs\n",
    "\n",
    "def classify_user_binary(user_dict, binary_models=binary_models):\n",
    "    emb = encode_posts_mean(user_dict.get(\"posts\",\"\"))\n",
    "    top_mbti, top_conf, axis_probs, mbti_probs = predict_mbti_from_embedding(emb, binary_models)\n",
    "\n",
    "    # aggregate role probabilities (keeping original design: use max if multiple MBTI map to same sec role)\n",
    "    sec_role_probs = {}\n",
    "    main_role_probs = {}\n",
    "    for mbti, p in mbti_probs.items():\n",
    "        secs = mbti_roles.get(mbti, [])\n",
    "        for sec in secs:\n",
    "            sec_role_probs[sec] = max(sec_role_probs.get(sec, 0.0), p)\n",
    "            main = role_to_main.get(sec)\n",
    "            if main:\n",
    "                main_role_probs[main] = max(main_role_probs.get(main, 0.0), p)\n",
    "    for c in CORE_ROLE_NAMES:\n",
    "        main_role_probs.setdefault(c, 0.0)\n",
    "\n",
    "    # top secondary roles\n",
    "    sec_sorted = sorted(sec_role_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "    sec_top = [s for s,_ in sec_sorted[:2]]\n",
    "\n",
    "    return {\n",
    "        \"name\": user_dict.get(\"name\"),\n",
    "        \"top_mbti\": top_mbti,\n",
    "        \"top_conf\": float(top_conf),\n",
    "        \"axis_probs\": axis_probs,\n",
    "        \"mbti_probs\": mbti_probs,\n",
    "        \"role_probs\": main_role_probs,\n",
    "        \"sec_roles_top\": sec_top\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2848d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Parse .txt with format: \"Name, post1 ||| post2 ||| ...\"\n",
    "def load_users_from_txt(path):\n",
    "    # accepts path or file-like (tempfile)\n",
    "    if hasattr(path, \"name\"):\n",
    "        fp = path.name\n",
    "    else:\n",
    "        fp = path\n",
    "    users = []\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln: continue\n",
    "            # Split on first comma to allow commas in name\n",
    "            if \",\" in ln:\n",
    "                name, rest = ln.split(\",\", 1)\n",
    "            else:\n",
    "                # fallback: assign generic name\n",
    "                name = ln[:30]\n",
    "                rest = ln\n",
    "            posts = rest.strip()\n",
    "            users.append({\"name\": name.strip(), \"posts\": posts})\n",
    "    return users\n",
    "\n",
    "# Quick test (uncomment to test)\n",
    "# print(load_users_from_txt(\"sample.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a32ae8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - grouping (core-first)\n",
    "def pick_best_for_role(candidates, role):\n",
    "    # candidates: list of enriched user dicts (with role_probs, top_conf)\n",
    "    best = None\n",
    "    best_score = -1.0\n",
    "    for u in candidates:\n",
    "        score = u[\"role_probs\"].get(role, 0.0)\n",
    "        # tie-breaker: top_conf\n",
    "        if score > best_score or (abs(score-best_score) < 1e-9 and u[\"top_conf\"] > (best[\"top_conf\"] if best else 0)):\n",
    "            best = u; best_score = score\n",
    "    return best, best_score\n",
    "\n",
    "def group_users_core_first(enriched_users, team_size=4):\n",
    "    \"\"\"\n",
    "    enriched_users: list of dicts returned by classify_user_binary\n",
    "    team_size: int (max 20)\n",
    "    returns assigned list of dicts: name, assigned_role, why, top_mbti, top_conf, role_prob\n",
    "    \"\"\"\n",
    "    team_size = min(int(team_size), 20)\n",
    "    users = list(enriched_users)  \n",
    "    assigned = []\n",
    "    assigned_names = set()\n",
    "\n",
    "    note = None\n",
    "    if team_size < 4:\n",
    "        note = \"⚠️ Team nhỏ hơn tiêu chuẩn, thiếu core roles.\"\n",
    "\n",
    "    # STEP 1: Đảm bảo 4 core roles (nếu có đủ người)\n",
    "    core_needed = CORE_ROLE_NAMES.copy()\n",
    "    remaining = users[:]\n",
    "\n",
    "    for core in core_needed:\n",
    "        if len(assigned) >= team_size:\n",
    "            break\n",
    "        cand, score = pick_best_for_role(remaining, core)\n",
    "        if cand:\n",
    "            assigned.append({\n",
    "                \"name\": cand[\"name\"],\n",
    "                \"assigned_role\": core,\n",
    "                \"why\": f\"Best match for {core} (prob {score:.2f})\",\n",
    "                \"top_mbti\": cand[\"top_mbti\"],\n",
    "                \"top_conf\": cand[\"top_conf\"],\n",
    "                \"role_prob\": score\n",
    "            })\n",
    "            assigned_names.add(cand[\"name\"])\n",
    "            remaining = [u for u in remaining if u[\"name\"] != cand[\"name\"]]\n",
    "\n",
    "    # STEP 2: Ràng buộc số lượng Leader\n",
    "    max_leaders = 1 if team_size < 8 else 3\n",
    "    current_leaders = sum(1 for a in assigned if a[\"assigned_role\"] == \"Leader\")\n",
    "\n",
    "    # STEP 3: Lấp đầy slot còn lại\n",
    "    slots_left = team_size - len(assigned)\n",
    "    if slots_left > 0:\n",
    "        ranked = []\n",
    "        for u in remaining:\n",
    "            main, rscore = max(u[\"role_probs\"].items(), key=lambda x: x[1])\n",
    "            ranked.append((u, main, rscore))\n",
    "        ranked.sort(key=lambda x: (-x[2], -x[0][\"top_conf\"], x[0][\"name\"]))\n",
    "\n",
    "        for u, main, rscore in ranked:\n",
    "            if len(assigned) >= team_size:\n",
    "                break\n",
    "            # enforce leader constraint\n",
    "            if main == \"Leader\" and current_leaders >= max_leaders:\n",
    "                continue\n",
    "            assigned.append({\n",
    "                \"name\": u[\"name\"],\n",
    "                \"assigned_role\": main,\n",
    "                \"why\": f\"Fill slot with {main} (prob {rscore:.2f})\",\n",
    "                \"top_mbti\": u[\"top_mbti\"],\n",
    "                \"top_conf\": u[\"top_conf\"],\n",
    "                \"role_prob\": rscore\n",
    "            })\n",
    "            assigned_names.add(u[\"name\"])\n",
    "            if main == \"Leader\":\n",
    "                current_leaders += 1\n",
    "\n",
    "    # STEP 4: Nếu còn slot (hiếm khi) → gán secondary role\n",
    "    slots_left = team_size - len(assigned)\n",
    "    if slots_left > 0:\n",
    "        for u in remaining:\n",
    "            if len(assigned) >= team_size:\n",
    "                break\n",
    "            sec = u[\"sec_roles_top\"][0] if u[\"sec_roles_top\"] else \"Supporter\"\n",
    "            assigned.append({\n",
    "                \"name\": u[\"name\"],\n",
    "                \"assigned_role\": sec,\n",
    "                \"why\": f\"Assigned secondary role {sec} (no core slot left)\",\n",
    "                \"top_mbti\": u[\"top_mbti\"],\n",
    "                \"top_conf\": u[\"top_conf\"],\n",
    "                \"role_prob\": u[\"role_probs\"].get(role_to_main.get(sec, sec), 0.0)\n",
    "            })\n",
    "            assigned_names.add(u[\"name\"])\n",
    "\n",
    "    # Add team note nếu cần\n",
    "    if note:\n",
    "        for a in assigned:\n",
    "            a[\"why\"] = note + \" \" + a[\"why\"]\n",
    "\n",
    "    return assigned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27db51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 - process pipeline for uploaded file\n",
    "def _ensure_filepath(file_input):\n",
    "    # file_input may be path string or file object (gradio returns TemporaryFile)\n",
    "    if file_input is None:\n",
    "        raise ValueError(\"No file provided.\")\n",
    "    if isinstance(file_input, str) and os.path.exists(file_input):\n",
    "        return file_input\n",
    "    # file-like\n",
    "    if hasattr(file_input, \"name\") and os.path.exists(file_input.name):\n",
    "        return file_input.name\n",
    "    # else try to write bytes to temp file if it's a dict (gradio older)\n",
    "    if isinstance(file_input, dict) and \"name\" in file_input and \"data\" in file_input:\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\")\n",
    "        tmp.write(file_input[\"data\"])\n",
    "        tmp.close()\n",
    "        return tmp.name\n",
    "    raise ValueError(\"Cannot resolve uploaded file path. Received object: \" + str(type(file_input)))\n",
    "\n",
    "def process_file_and_assign(uploaded_file, team_size=4, translate_enable=False):\n",
    "    \"\"\"\n",
    "    Returns (markdown_str, pandas.DataFrame) for Gradio outputs\n",
    "    \"\"\"\n",
    "    # safety\n",
    "    team_size = min(int(team_size), 20)\n",
    "    fp = _ensure_filepath(uploaded_file)\n",
    "    users = load_users_from_txt(fp)\n",
    "    if not users:\n",
    "        return \"Không đọc được user từ file.\", pd.DataFrame()\n",
    "\n",
    "    # optional translate & classify all\n",
    "    enriched = []\n",
    "    for u in users:\n",
    "        posts = u[\"posts\"]\n",
    "        if translate_enable:\n",
    "            posts = maybe_translate(posts, enable_translate=True, target=\"en\")\n",
    "        u2 = {\"name\": u[\"name\"], \"posts\": posts}\n",
    "        res = classify_user_binary(u2, binary_models=binary_models)\n",
    "        enriched.append(res)\n",
    "\n",
    "    # group\n",
    "    assigned = group_users_core_first(enriched, team_size=team_size)\n",
    "\n",
    "    # format markdown\n",
    "    md = \"| # | Name | Assigned Role | Reason | MBTI (conf) | RoleProb |\\n\"\n",
    "    md += \"|---:|---|---|---|---|---|\\n\"\n",
    "    for i, a in enumerate(assigned, start=1):\n",
    "        md += f\"| {i} | {a['name']} | {a['assigned_role']} | {a['why']} | {a['top_mbti']} ({a['top_conf']:.2f}) | {a['role_prob']:.2f} |\\n\"\n",
    "\n",
    "    # also prepare DataFrame\n",
    "    df = pd.DataFrame(assigned)\n",
    "    return md, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "365ab24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 10 - Gradio UI (paste & run)\n",
    "if gr is None:\n",
    "    print(\"Gradio not installed. Install with `pip install gradio` to launch UI.\")\n",
    "else:\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## 🧠 MBTI Team Builder — Core-first (Binary models)\")\n",
    "        gr.Markdown(\"Upload `.txt` file with lines: `Name, post1 ||| post2 ||| ...`\")\n",
    "        with gr.Row():\n",
    "            file_input = gr.File(label=\"Upload .txt file\", file_types=[\".txt\"])\n",
    "            team_slider = gr.Slider(minimum=1, maximum=20, step=1, value=4, label=\"Team size (max 20)\")\n",
    "            translate_chk = gr.Checkbox(value=False, label=\"Auto-translate posts to English (if not English)\")\n",
    "        run_btn = gr.Button(\"Assign Team\")\n",
    "        md_out = gr.Markdown()\n",
    "        df_out = gr.Dataframe(headers=[\"name\",\"assigned_role\",\"why\",\"top_mbti\",\"top_conf\",\"role_prob\"], label=\"Assigned members (table)\")\n",
    "\n",
    "        def _wrap(file, team_size, translate):\n",
    "            try:\n",
    "                md, df = process_file_and_assign(file, team_size=team_size, translate_enable=translate)\n",
    "                return md, df\n",
    "            except Exception as e:\n",
    "                return f\"Error: {e}\", pd.DataFrame()\n",
    "\n",
    "        run_btn.click(fn=_wrap, inputs=[file_input, team_slider, translate_chk], outputs=[md_out, df_out])\n",
    "\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22e32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
